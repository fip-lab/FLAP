////////////////////////////////////////////////////////////////////
// Skill Teaching Domain 
//
// Author: Tom Walsh (thomasjwalsh [at] gmail.com)
// Special thanks to Derek Green and Paul Cohen at 
// University of Arizona for help with the design.
//
// In the SkillTeaching MDP domain, the agent is trying to teach a series 
// of skills to a student through the use of hints and multiple choice
// questions.  The student has a proficiency level for each skill, which 
// indicates his ability to answer questions of that skill and positive
// reward is given for high proficiency on skills while negative reward 
// is given for low proficiency.  Each skill also has a weight on 
// how much it is worth. 
//
//  Many of the skills are connected in that some are
// ``pre-conditions'' of others.  If all of a skill's 
// pre-conditions are learned, the student has some probability 
// of answering questions about it right, and each precondition
// that is at high proficiency adds to the probability though 
// knowing all of them can lead to a probability higher than the sum
// of the components.  Hints only work if all the preconditions 
// are known and can only get you to medium proficiency.
//
// student proficiency increases with questions answered right and 
// decreases with questions about a skill answered wrong and 
// sometimes decreases by chance.
//
// To model the teacher-student interaction, every other step in the
// domain is the student's turn, where they answer a question.  
//
// The planning problems here are:
// 1) Whether or not to teach all the prerequisites of a skill before
//    teaching it.
// 2) What skill to focus on next
// 3) When to give hints and when to use multiple choice problems
//
////////////////////////////////////////////////////////////////////

domain skill_teaching_mdp {
  	
	requirements = { 
		reward-deterministic,
		preconditions
	};

	types { 
		skill : object;
	};
      	
	pvariables { 
		
		//how valuable is this skill?		
		SKILL_WEIGHT(skill) : { non-fluent, real, default = 1.0 };
		
		//some skills are pre-reqs for others.  Your ability to achiev a higher level skill is dependent on how 
		//many of the pre-reqs you have mastered
		PRE_REQ(skill, skill) : { non-fluent, bool, default = false };

		//probability of getting a question right if you have all the pre-reqs
		PROB_ALL_PRE(skill) : { non-fluent, real, default = 0.8 };
		//if you don't have all the pre-cons, probaility mass is summed using these individual pieces
		PROB_PER_PRE(skill) : { non-fluent, real, default = 0.1 };

		PROB_ALL_PRE_MED(skill) : { non-fluent, real, default = 1.0 };
		//if you don't have all the pre-cons, probaility mass is summed using these individual pieces
		PROB_PER_PRE_MED(skill) : { non-fluent, real, default = 0.3 };
		
		PROB_HIGH(skill) : { non-fluent, real, default = 0.9 };

		LOSE_PROB(skill) : { non-fluent, real, default = 0.02 };

		//proficiency values, they accumulate so low and med can be on at the same time and only high will turn off
		proficiencyMed(skill) : { state-fluent, bool, default = false };
		proficiencyHigh(skill) : { state-fluent, bool, default = false };

		updateTurn(skill) : {state-fluent, bool, default = false};

		answeredRight(skill): {state-fluent, bool, default = false};
		hintedRight(skill): {state-fluent, bool, default = false};
		hintDelayVar(skill) : {state-fluent, bool, default = false};
		
		//two actions.  Hint can get you directly to proficiencyMed, but only if all the pre_reqs are on
		askProb(skill) : {action-fluent, bool, default = false};
		giveHint(skill) :  {action-fluent, bool, default = false};
	};
  
	cpfs {

		updateTurn'(?s) = 
			KronDelta( [forall_{?s2: skill} ~updateTurn(?s2)] ^ (askProb(?s) | giveHint(?s)) ); 

		//without intermediate nodes, we need to keep ``on'' all proficiency levels that have been attained		

		answeredRight'(?s) = 
			if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s) ^ proficiencyHigh(?s)) 
				then Bernoulli(PROB_HIGH(?s))
			else if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s) ^ proficiencyMed(?s) ^forall_{?s3: skill}[PRE_REQ(?s3, ?s) => proficiencyHigh(?s3)]) 
				then Bernoulli(PROB_ALL_PRE_MED(?s))
		    else if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s) ^proficiencyMed(?s) ^ askProb(?s)) 
		    	then Bernoulli(sum_{?s2: skill}[PRE_REQ(?s2, ?s) * PROB_PER_PRE_MED(?s)])
			else if ([forall_{?s3: skill} ~updateTurn(?s3)] ^ askProb(?s) ^forall_{?s2: skill}[PRE_REQ(?s2, ?s) => proficiencyHigh(?s2)]) 
				then Bernoulli(PROB_ALL_PRE(?s))
		    else if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s)  ^ askProb(?s)) 
		    	then Bernoulli(sum_{?s2: skill}[PRE_REQ(?s2, ?s) * PROB_PER_PRE(?s)])
			else
				KronDelta( false );

		hintedRight'(?s) = 
			KronDelta( [forall_{?s3: skill} ~updateTurn(?s3)] ^ giveHint(?s) ^ forall_{?s2: skill}[PRE_REQ(?s2, ?s) => proficiencyHigh(?s2)] );
			
		hintDelayVar'(?s) = 
			KronDelta( [forall_{?s2: skill} ~updateTurn(?s2)] ^ giveHint(?s) );

		//proficiencyMed can be reached through a hint if all preconditions are known or by a problem answered correctly
		proficiencyMed'(?s) =
		    if (~updateTurn(?s) ^ proficiencyMed(?s)) 
		    	then KronDelta( true )
		    else if (updateTurn(?s) ^ hintedRight(?s)) 
		    	then KronDelta( true )
		    else if (updateTurn(?s) ^ answeredRight(?s)) 
		    	then KronDelta( true )
		    else if (proficiencyHigh(?s)) //may come down
		    	then KronDelta( true )
		    else if (proficiencyMed(?s) ^ updateTurn(?s) ^ hintDelayVar(?s)) 
		    	then KronDelta( true ) //can't lose it on a hint
	        else 
	        	KronDelta( false );

		//high proficiency is reached by getting a question and having proficiencyMed
		//but you can lose it too if you get questions wrong  
		proficiencyHigh'(?s) =
		    if (forall_{?s2: skill}[~updateTurn(?s2)])  //student turn
		    	then KronDelta( proficiencyHigh(?s) )
		    else if (~updateTurn(?s) ^ proficiencyHigh(?s)) 
		    	then Bernoulli(1.0 - LOSE_PROB(?s))
		    else if (proficiencyMed(?s) ^ updateTurn(?s) ^ answeredRight(?s)) 
		    	then KronDelta( true )
		    else if (proficiencyHigh(?s) ^ updateTurn(?s) ^ (hintDelayVar(?s) | answeredRight(?s))) //can't lose it on a hint
		    	then KronDelta( true )
		    else KronDelta( false );

	};
    
	 reward = [sum_{?s : skill} [SKILL_WEIGHT(?s) * proficiencyHigh(?s)]] + [sum_{?s : skill} -[SKILL_WEIGHT(?s) * ~proficiencyMed(?s)]];
		action-preconditions {
        (sum_{?s : skill} [ askProb(?s) + giveHint(?s) ]) <= 1;
    };
}







instance skill_teaching_inst_mdp__8 { 
	domain = skill_teaching_mdp; 
	objects { 
		skill : {s0,s1,s2,s3,s4,s5,s6};

	}; 
	non-fluents {
		PROB_ALL_PRE(s0) = 0.6240561;
		PROB_ALL_PRE_MED(s0) = 0.6767317;
		PROB_HIGH(s0) = 0.99838704;
		SKILL_WEIGHT(s0) = 1.41502;
		LOSE_PROB(s0) = 0.013960319757461549;
		PROB_ALL_PRE(s1) = 0.7315788;
		PROB_ALL_PRE_MED(s1) = 0.7984441;
		PROB_HIGH(s1) = 0.9399337;
		SKILL_WEIGHT(s1) = 1.2734425;
		LOSE_PROB(s1) = 0.01895264983177185;
		PRE_REQ(s0, s2);
		PRE_REQ(s1, s2);
		PROB_ALL_PRE(s2) = 0.60257506;
		PROB_PER_PRE(s2) = 0.24754523038864135;
		PROB_ALL_PRE_MED(s2) = 0.771655;
		PROB_PER_PRE_MED(s2) = 0.29233548045158386;
		PROB_HIGH(s2) = 0.8795395;
		SKILL_WEIGHT(s2) = 2.160461;
		LOSE_PROB(s2) = 0.018922489881515504;
		PRE_REQ(s2, s3);
		PRE_REQ(s1, s3);
		PROB_ALL_PRE(s3) = 0.5341582;
		PROB_PER_PRE(s3) = 0.23814212083816527;
		PROB_ALL_PRE_MED(s3) = 0.78818285;
		PROB_PER_PRE_MED(s3) = 0.3877822756767273;
		PROB_HIGH(s3) = 0.91199195;
		SKILL_WEIGHT(s3) = 3.3209429;
		LOSE_PROB(s3) = 0.03833227455615998;
		PRE_REQ(s1, s4);
		PRE_REQ(s2, s4);
		PRE_REQ(s0, s4);
		PROB_ALL_PRE(s4) = 0.6759284;
		PROB_PER_PRE(s4) = 0.17441368997097015;
		PROB_ALL_PRE_MED(s4) = 0.7965332;
		PROB_PER_PRE_MED(s4) = 0.23855116367340087;
		PROB_HIGH(s4) = 0.9278462;
		SKILL_WEIGHT(s4) = 3.1534872;
		LOSE_PROB(s4) = 0.036063560843467714;
		PRE_REQ(s3, s5);
		PRE_REQ(s4, s5);
		PRE_REQ(s0, s5);
		PROB_ALL_PRE(s5) = 0.50205016;
		PROB_PER_PRE(s5) = 0.09400476813316345;
		PROB_ALL_PRE_MED(s5) = 0.6836033;
		PROB_PER_PRE_MED(s5) = 0.13496833145618437;
		PROB_HIGH(s5) = 0.9482524;
		SKILL_WEIGHT(s5) = 4.2396054;
		LOSE_PROB(s5) = 0.01031070649623871;
		PRE_REQ(s5, s6);
		PROB_ALL_PRE(s6) = 0.5515639;
		PROB_PER_PRE(s6) = 0.529619014263153;
		PROB_ALL_PRE_MED(s6) = 0.6806831;
		PROB_PER_PRE_MED(s6) = 0.6520019173622131;
		PROB_HIGH(s6) = 0.9999735;
		SKILL_WEIGHT(s6) = 5.047902;
		LOSE_PROB(s6) = 0.02086797058582306;
	};
	init-state{
		proficiencyMed(s0);
	};
	horizon = 40;
	discount = 1.0;
}




