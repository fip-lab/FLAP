////////////////////////////////////////////////////////////////////
// Skill Teaching Domain 
//
// Author: Tom Walsh (thomasjwalsh [at] gmail.com)
// Special thanks to Derek Green and Paul Cohen at 
// University of Arizona for help with the design.
//
// In the SkillTeaching MDP domain, the agent is trying to teach a series 
// of skills to a student through the use of hints and multiple choice
// questions.  The student has a proficiency level for each skill, which 
// indicates his ability to answer questions of that skill and positive
// reward is given for high proficiency on skills while negative reward 
// is given for low proficiency.  Each skill also has a weight on 
// how much it is worth. 
//
//  Many of the skills are connected in that some are
// ``pre-conditions'' of others.  If all of a skill's 
// pre-conditions are learned, the student has some probability 
// of answering questions about it right, and each precondition
// that is at high proficiency adds to the probability though 
// knowing all of them can lead to a probability higher than the sum
// of the components.  Hints only work if all the preconditions 
// are known and can only get you to medium proficiency.
//
// student proficiency increases with questions answered right and 
// decreases with questions about a skill answered wrong and 
// sometimes decreases by chance.
//
// To model the teacher-student interaction, every other step in the
// domain is the student's turn, where they answer a question.  
//
// The planning problems here are:
// 1) Whether or not to teach all the prerequisites of a skill before
//    teaching it.
// 2) What skill to focus on next
// 3) When to give hints and when to use multiple choice problems
//
////////////////////////////////////////////////////////////////////

domain skill_teaching_mdp {
  	
	requirements = { 
		reward-deterministic,
		preconditions
	};

	types { 
		skill : object;
	};
      	
	pvariables { 
		
		//how valuable is this skill?		
		SKILL_WEIGHT(skill) : { non-fluent, real, default = 1.0 };
		
		//some skills are pre-reqs for others.  Your ability to achiev a higher level skill is dependent on how 
		//many of the pre-reqs you have mastered
		PRE_REQ(skill, skill) : { non-fluent, bool, default = false };

		//probability of getting a question right if you have all the pre-reqs
		PROB_ALL_PRE(skill) : { non-fluent, real, default = 0.8 };
		//if you don't have all the pre-cons, probaility mass is summed using these individual pieces
		PROB_PER_PRE(skill) : { non-fluent, real, default = 0.1 };

		PROB_ALL_PRE_MED(skill) : { non-fluent, real, default = 1.0 };
		//if you don't have all the pre-cons, probaility mass is summed using these individual pieces
		PROB_PER_PRE_MED(skill) : { non-fluent, real, default = 0.3 };
		
		PROB_HIGH(skill) : { non-fluent, real, default = 0.9 };

		LOSE_PROB(skill) : { non-fluent, real, default = 0.02 };

		//proficiency values, they accumulate so low and med can be on at the same time and only high will turn off
		proficiencyMed(skill) : { state-fluent, bool, default = false };
		proficiencyHigh(skill) : { state-fluent, bool, default = false };

		updateTurn(skill) : {state-fluent, bool, default = false};

		answeredRight(skill): {state-fluent, bool, default = false};
		hintedRight(skill): {state-fluent, bool, default = false};
		hintDelayVar(skill) : {state-fluent, bool, default = false};
		
		//two actions.  Hint can get you directly to proficiencyMed, but only if all the pre_reqs are on
		askProb(skill) : {action-fluent, bool, default = false};
		giveHint(skill) :  {action-fluent, bool, default = false};
	};
  
	cpfs {

		updateTurn'(?s) = 
			KronDelta( [forall_{?s2: skill} ~updateTurn(?s2)] ^ (askProb(?s) | giveHint(?s)) ); 

		//without intermediate nodes, we need to keep ``on'' all proficiency levels that have been attained		

		answeredRight'(?s) = 
			if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s) ^ proficiencyHigh(?s)) 
				then Bernoulli(PROB_HIGH(?s))
			else if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s) ^ proficiencyMed(?s) ^forall_{?s3: skill}[PRE_REQ(?s3, ?s) => proficiencyHigh(?s3)]) 
				then Bernoulli(PROB_ALL_PRE_MED(?s))
		    else if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s) ^proficiencyMed(?s) ^ askProb(?s)) 
		    	then Bernoulli(sum_{?s2: skill}[PRE_REQ(?s2, ?s) * PROB_PER_PRE_MED(?s)])
			else if ([forall_{?s3: skill} ~updateTurn(?s3)] ^ askProb(?s) ^forall_{?s2: skill}[PRE_REQ(?s2, ?s) => proficiencyHigh(?s2)]) 
				then Bernoulli(PROB_ALL_PRE(?s))
		    else if ([forall_{?s2: skill} ~updateTurn(?s2)] ^ askProb(?s)  ^ askProb(?s)) 
		    	then Bernoulli(sum_{?s2: skill}[PRE_REQ(?s2, ?s) * PROB_PER_PRE(?s)])
			else
				KronDelta( false );

		hintedRight'(?s) = 
			KronDelta( [forall_{?s3: skill} ~updateTurn(?s3)] ^ giveHint(?s) ^ forall_{?s2: skill}[PRE_REQ(?s2, ?s) => proficiencyHigh(?s2)] );
			
		hintDelayVar'(?s) = 
			KronDelta( [forall_{?s2: skill} ~updateTurn(?s2)] ^ giveHint(?s) );

		//proficiencyMed can be reached through a hint if all preconditions are known or by a problem answered correctly
		proficiencyMed'(?s) =
		    if (~updateTurn(?s) ^ proficiencyMed(?s)) 
		    	then KronDelta( true )
		    else if (updateTurn(?s) ^ hintedRight(?s)) 
		    	then KronDelta( true )
		    else if (updateTurn(?s) ^ answeredRight(?s)) 
		    	then KronDelta( true )
		    else if (proficiencyHigh(?s)) //may come down
		    	then KronDelta( true )
		    else if (proficiencyMed(?s) ^ updateTurn(?s) ^ hintDelayVar(?s)) 
		    	then KronDelta( true ) //can't lose it on a hint
	        else 
	        	KronDelta( false );

		//high proficiency is reached by getting a question and having proficiencyMed
		//but you can lose it too if you get questions wrong  
		proficiencyHigh'(?s) =
		    if (forall_{?s2: skill}[~updateTurn(?s2)])  //student turn
		    	then KronDelta( proficiencyHigh(?s) )
		    else if (~updateTurn(?s) ^ proficiencyHigh(?s)) 
		    	then Bernoulli(1.0 - LOSE_PROB(?s))
		    else if (proficiencyMed(?s) ^ updateTurn(?s) ^ answeredRight(?s)) 
		    	then KronDelta( true )
		    else if (proficiencyHigh(?s) ^ updateTurn(?s) ^ (hintDelayVar(?s) | answeredRight(?s))) //can't lose it on a hint
		    	then KronDelta( true )
		    else KronDelta( false );

	};
    
	 reward = [sum_{?s : skill} [SKILL_WEIGHT(?s) * proficiencyHigh(?s)]] + [sum_{?s : skill} -[SKILL_WEIGHT(?s) * ~proficiencyMed(?s)]];
		action-preconditions {
        (sum_{?s : skill} [ askProb(?s) + giveHint(?s) ]) <= 1;
    };
}







instance skill_teaching_inst_mdp__10 { 
	domain = skill_teaching_mdp; 
	objects { 
		skill : {s0,s1,s2,s3,s4,s5,s6,s7};

	}; 
	non-fluents {
		PROB_ALL_PRE(s0) = 0.5379598;
		PROB_ALL_PRE_MED(s0) = 0.7966592;
		PROB_HIGH(s0) = 0.86250937;
		SKILL_WEIGHT(s0) = 1.3225485;
		LOSE_PROB(s0) = 0.04723575413227082;
		PROB_ALL_PRE(s1) = 0.5750398;
		PROB_ALL_PRE_MED(s1) = 0.75465524;
		PROB_HIGH(s1) = 0.9738445;
		SKILL_WEIGHT(s1) = 1.3837464;
		LOSE_PROB(s1) = 0.031081205606460573;
		PRE_REQ(s0, s2);
		PRE_REQ(s1, s2);
		PROB_ALL_PRE(s2) = 0.66695064;
		PROB_PER_PRE(s2) = 0.29437217116355896;
		PROB_ALL_PRE_MED(s2) = 0.7140531;
		PROB_PER_PRE_MED(s2) = 0.3180202007293701;
		PROB_HIGH(s2) = 0.91557336;
		SKILL_WEIGHT(s2) = 2.206152;
		LOSE_PROB(s2) = 0.012146511673927309;
		PRE_REQ(s2, s3);
		PRE_REQ(s0, s3);
		PRE_REQ(s1, s3);
		PROB_ALL_PRE(s3) = 0.62654245;
		PROB_PER_PRE(s3) = 0.19705301225185395;
		PROB_ALL_PRE_MED(s3) = 0.78798556;
		PROB_PER_PRE_MED(s3) = 0.24863892197608947;
		PROB_HIGH(s3) = 0.86368424;
		SKILL_WEIGHT(s3) = 3.1999547;
		LOSE_PROB(s3) = 0.01;
		PRE_REQ(s3, s4);
		PRE_REQ(s1, s4);
		PRE_REQ(s0, s4);
		PROB_ALL_PRE(s4) = 0.73045695;
		PROB_PER_PRE(s4) = 0.14810273945331573;
		PROB_ALL_PRE_MED(s4) = 0.73045695;
		PROB_PER_PRE_MED(s4) = 0.14849880039691926;
		PROB_HIGH(s4) = 0.850513;
		SKILL_WEIGHT(s4) = 4.0810323;
		LOSE_PROB(s4) = 0.020675605535507204;
		PRE_REQ(s0, s5);
		PRE_REQ(s3, s5);
		PRE_REQ(s1, s5);
		PROB_ALL_PRE(s5) = 0.5595663;
		PROB_PER_PRE(s5) = 0.15677647292613983;
		PROB_ALL_PRE_MED(s5) = 0.74796265;
		PROB_PER_PRE_MED(s5) = 0.20301721394062042;
		PROB_HIGH(s5) = 0.946712;
		SKILL_WEIGHT(s5) = 4.3929977;
		LOSE_PROB(s5) = 0.01;
		PRE_REQ(s1, s6);
		PROB_ALL_PRE(s6) = 0.58252585;
		PROB_PER_PRE(s6) = 0.5016196370124817;
		PROB_ALL_PRE_MED(s6) = 0.68700767;
		PROB_PER_PRE_MED(s6) = 0.6655245840549469;
		PROB_HIGH(s6) = 0.94299585;
		SKILL_WEIGHT(s6) = 2.0293026;
		LOSE_PROB(s6) = 0.01;
		PRE_REQ(s5, s7);
		PROB_ALL_PRE(s7) = 0.56091326;
		PROB_PER_PRE(s7) = 0.5188458919525146;
		PROB_ALL_PRE_MED(s7) = 0.7564045;
		PROB_PER_PRE_MED(s7) = 0.6960887908935547;
		PROB_HIGH(s7) = 0.9716682;
		SKILL_WEIGHT(s7) = 5.129872;
		LOSE_PROB(s7) = 0.031601646542549135;
	};
	init-state{
		proficiencyMed(s0);
	};
	horizon = 40;
	discount = 1.0;
}




